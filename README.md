deme

---

# ğŸš•ğŸ“Š NYC Taxi & Weather Analytics ETL Pipeline

---

## ğŸ“Œâœ¨ Project Overview

This project designs and implements a **scalable, resilient ETL pipeline** that integrates large-scale **NYC taxi transportation data** with **real-time weather information**.

Using **âš¡ Apache PySpark** for distributed data processing, **ğŸ¦† DuckDB** for high-performance analytics, and **ğŸ§­ Prefect** for orchestration, the pipeline extracts, transforms, and loads heterogeneous data sources into an **analytics-ready database**.

The final curated dataset is visualized using a **ğŸ“Š Business Intelligence dashboard** (Power BI / Tableau / Looker) to support **data-driven decision-making**.

---

## ğŸ¯ğŸ“ˆ Business Problem

Urban transportation demand is influenced by **â° time**, **ğŸ“ location**, and **ğŸŒ¦ï¸ weather conditions**.
This project answers the following key analytical questions:

* ğŸš• How do taxi trips vary by **hour**, **weekday**, and **borough**?
* ğŸŒ§ï¸ How does **weather** (rain, snow, temperature) affect **trip duration and demand**?
* ğŸ“† Are there **peak travel patterns** during weekends or adverse weather conditions?

---

## ğŸ§©ğŸ“‚ Data Sources

| Source                   | Format     | Description                               |
| ------------------------ | ---------- | ----------------------------------------- |
| ğŸš• NYC Taxi Trip Records | Parquet    | High-volume taxi trip transaction data    |
| ğŸ—ºï¸ Taxi Zone Lookup     | CSV        | Borough and location mapping              |
| ğŸŒ¦ï¸ NYC Weather Data     | JSON (API) | Hourly temperature and weather conditions |

âœ” **Requirement satisfied:** Includes at least one **Parquet-based** data source.

---

## ğŸ—ï¸ğŸ§  Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Parquet Data â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CSV Data   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚  JSON (API)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Apache PySpark (Transform)     â”‚
â”‚ - Cleaning                     â”‚
â”‚ - Enrichment                   â”‚
â”‚ - Feature Engineering          â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Parquet (Staging)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DuckDB (Analytics) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BI Dashboard       â”‚
â”‚ (Power BI / etc.)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ğŸ› ï¸ Technologies Used

| Category                 | Tool                        |
| ------------------------ | --------------------------- |
| âš¡ Distributed Processing | Apache PySpark              |
| ğŸ§­ Orchestration         | Prefect                     |
| ğŸ’¾ Storage               | Parquet                     |
| ğŸ¦† Analytics Database    | DuckDB                      |
| ğŸ“Š BI Tool               | Power BI / Tableau / Looker |
| ğŸ Programming Language  | Python 3                    |
| ğŸªŸ OS Support            | Windows-safe configuration  |

---

## ğŸ”„ğŸ”§ ETL Pipeline Description

### 1ï¸âƒ£ ğŸ“¥ Extract

* Read **Parquet-based** NYC taxi trip records
* Load taxi zone lookup data from **CSV**
* Retrieve and parse weather data from a **JSON-based API**

---

### 2ï¸âƒ£ ğŸ”„ Transform (Apache PySpark)

* âŒ Filter invalid taxi trips (distance, fare, duration)
* ğŸ§¹ Handle missing and null values safely
* ğŸ§  Feature engineering:

  * â±ï¸ Trip duration
  * ğŸ•’ Pickup hour, weekday, weekend flag
  * ğŸŒ¦ï¸ Weather category
* ğŸ”— Enrich taxi trips with:

  * ğŸ—ºï¸ Borough information
  * â° Hourly weather data
* ğŸ›¡ï¸ Apply fallback logic for missing weather records

---

### 3ï¸âƒ£ ğŸ“¤ Load

* Write transformed datasets to **Parquet staging storage**
* Load Parquet files into **DuckDB** using `read_parquet()`
* Create **analytics-ready tables** for BI consumption

---

## â±ï¸ğŸ§­ Pipeline Orchestration (Prefect)

The ETL pipeline is orchestrated using **Prefect**, providing:

* ğŸ” Task-level fault tolerance
* â™»ï¸ Automatic retries with delay
* ğŸ§  Daily caching to avoid redundant computation
* ğŸ”— Explicit task dependency management

The pipeline is structured as a **Prefect Flow**, with independent tasks for:

* PySpark transformation and enrichment
* DuckDB loading and validation

---

## â±ï¸ğŸ  Local Prefect Setup (Development & Academic Use)

### 1ï¸âƒ£ â–¶ï¸ Start Prefect Server

```powershell
prefect server start
```

UI available at:

```
http://127.0.0.1:4200
```

---

### 2ï¸âƒ£ ğŸ“‚ Navigate to Project Directory

```powershell
cd C:\Users\HP\Documents\etl-project
```

---

### 3ï¸âƒ£ ğŸ—ï¸ Create Work Pool (One-Time Setup)

```powershell
prefect work-pool create local-pool --type process
```

---

### 4ï¸âƒ£ ğŸ—“ï¸ Deploy the Flow with Schedule

```powershell
prefect deploy prefect_flow.py:main_flow --name daily_etl --pool local-pool --cron "0 9 * * *"
```

âœ” Scheduled to run **daily at 09:00**

---

### 5ï¸âƒ£ ğŸ§‘â€ğŸ’» Start Prefect Worker

```powershell
prefect worker start --pool local-pool
```

---

testing screenshots of the terminal
<img width="1460" height="427" alt="Screenshot 2025-12-15 045758" src="https://github.com/user-attachments/assets/c27108ff-9365-4f08-82f5-085cf9315b64" />

<img width="1316" height="403" alt="Screenshot 2025-12-15 044211" src="https://github.com/user-attachments/assets/5e9485fd-fa0f-4689-ad56-fcb91c1e0781" />


### 6ï¸âƒ£ ğŸ” Monitoring

Using the Prefect UI, you can:

* ğŸ‘€ Monitor flow execution
* ğŸ“œ Inspect logs
* ğŸ”„ Track retries and failures

---

## ğŸ“ŠğŸ–¼ï¸ BI Dashboard (With Images)

The DuckDB database is connected to a BI tool to visualize:

* â° Taxi trips by hour and weekday
* ğŸŒ¦ï¸ Average trip duration by weather condition
* ğŸ“† Demand comparison: weekday vs weekend
* ğŸŒ¡ï¸ Temperature impact on taxi usage

### ğŸ“¸ Dashboard Screenshots

> Place your images inside `bi/dashboard_screenshots/`

```md
![Taxi Trips by Hour](bi/dashboard_screenshots/trips_by_hour.png)
![Average Trip Duration by Weather](bi/dashboard_screenshots/trip_duration_weather.png)
![Weekday vs Weekend Demand](bi/dashboard_screenshots/weekday_weekend.png)
![Temperature Impact on Demand](bi/dashboard_screenshots/temperature_impact.png)
```

âœ… GitHub will render these images automatically once the files exist.

---
Perfect, Yonas ğŸ‘
You **do NOT need to remove or reduce anything** from your README.
Below is a **clean, polished, copy-paste ready Markdown section** that fits **exactly before** the **ğŸ“ğŸ—‚ï¸ Repository Structure** section.

I only **refined wording, headings, and formatting** â€” the meaning and content remain **100% yours**.

---

## ğŸ”§ğŸ“ dbt (Data Build Tool) Integration

### Overview

In addition to the core ETL pipeline, this project integrates **dbt (Data Build Tool)** to perform **advanced transformation modeling** directly within **DuckDB**.

dbt is used to organize SQL-based transformations into **structured, testable, and reusable data models**, following modern **analytics engineering best practices**.
This integration enhances **data quality**, **maintainability**, and **analytical clarity**, and fulfills the **bonus requirement** for advanced transformation modeling.

---

### Why dbt Was Used

While **Apache PySpark** handles large-scale data extraction, cleaning, and enrichment, **dbt is applied after data is loaded into DuckDB** to:

* Structure transformations into **staging and mart layers**
* Define **clear model dependencies**
* Enable **SQL-based analytical modeling**
* Improve **reproducibility and documentation**
* Separate **ETL processing** from **analytics logic**

This approach reflects **real-world data warehouse design patterns** used in industry.

---

### dbt Position in the Pipeline

```
Raw Data Sources (Parquet, CSV, JSON)
        â†“
PySpark Transformations (ETL)
        â†“
DuckDB (Processed Analytical Store)
        â†“
dbt Models (Staging â†’ Mart)
        â†“
BI Dashboard (Power BI)
```

âœ” dbt operates **on top of DuckDB** and **does not replace** the ETL pipeline.

---

### dbt Models Implemented

#### 1ï¸âƒ£ Staging Model

A staging model was created to standardize and expose cleaned data from DuckDB.

* **Model name:** `stg_taxi_weather`
* **Purpose:** Provide a clean, structured analytical view
* **Source:** DuckDB tables generated by the ETL pipeline

Key fields include:

* Trip distance
* Fare amount
* Trip duration
* Weather category
* Pickup date and time

---

#### 2ï¸âƒ£ Mart (Analytics) Model

An analytical mart model was created to summarize taxi activity.

* **Model name:** `mart_trips_summary`
* **Purpose:** Aggregate taxi trips by pickup location

**Metrics produced:**

* Total number of trips
* Average trip distance
* Average fare amount
* Average trip duration

This model demonstrates **advanced transformation modeling** and serves as a **direct input for BI dashboards**.

---

### Data Quality & Testing

Basic data quality validation was implemented using **dbt tests**, including:

* `not_null` checks on critical fields such as **`PULocationID`**

These tests ensure **data reliability** before analytics and reporting.

---

### Impact on Existing Dashboard

The dbt integration was designed to be **non-disruptive**:

* Existing **Power BI dashboards continue to work without modification**
* dbt-generated models can optionally be used as **optimized analytical sources**
* No changes were required to existing **ETL logic or DuckDB schemas**

---

### Benefits of dbt Integration

* Clear separation of **data engineering (ETL)** and **analytics engineering (modeling)**
* Improved readability and maintainability of SQL transformations
* Industry-aligned modeling practices
* Scalable and reusable analytics layer
* Satisfies the **bonus requirement** for advanced transformation modeling

---

### Summary

By integrating **dbt with DuckDB**, this project demonstrates a **complete modern data pipeline** that combines distributed processing, analytical storage, orchestration, and structured analytics modeling.
This design closely mirrors **real-world production data engineering workflows** and strengthens both **technical robustness** and **analytical usability**.

---



## ğŸ“ğŸ—‚ï¸ Repository Structure

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ taxi_parquet/
â”‚   â”‚   â”œâ”€â”€ taxi_zone_lookup.csv
â”‚   â”‚   â””â”€â”€ weather/
â”‚   â”œâ”€â”€ staging_parquet/
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ taxi_weather.duckdb
â”‚
â”œâ”€â”€ prefect_flow.py
â”œâ”€â”€ etl_pipeline.ipynb
â”œâ”€â”€ README.md
â”‚
â””â”€â”€ bi/
    â””â”€â”€ dashboard_screenshots/
```

---

## ğŸ§ªâœ… Data Quality Checks

* ğŸ” Null value validation
* âŒ Invalid trip filtering
* ğŸŒ¦ï¸ Weather fallback logic
* ğŸ§¾ Schema inspection
* ğŸ“Š Row and column verification in DuckDB

---

## ğŸ‘¥ğŸ¤ Team Members & Roles

| Name              | Role / Contribution                                            |
| ----------------- | -------------------------------------------------------------- |
| Mikiyas Tolko     | Project Lead; ETL architecture; PySpark; Prefect orchestration |
| Demirew Manidefro | DuckDB integration; schema validation                          |
| Lamrot Solomon    | Weather API integration; JSON parsing                          |
| Nahom Teshome     | Data cleaning; null handling; duration calculations            |
| Yonas Habtamu     | BI dashboard design; taxi zone mapping                         |
| Abaynewu Aberu    | Documentation; README preparation; coordination                |
| Yonas Abebe       | Testing; validation; data quality checks                       |

---

## ğŸš€â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ ğŸ“¦ Install Dependencies

```bash
pip install pyspark duckdb prefect findspark requests
```

### 2ï¸âƒ£ ğŸ” Set Environment Variable

```bash
export WEATHER_API_KEY="your_api_key"
```

### 3ï¸âƒ£ â–¶ï¸ Run ETL Pipeline

```bash
python prefect_flow.py
```

---
